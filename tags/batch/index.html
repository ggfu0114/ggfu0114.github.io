<!doctype html><html lang=zh-tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>batch` | GGfu Personal Study - 學習筆記</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content><meta name=generator content="Hugo 0.100.2"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link href=/tags/batch/index.xml rel=alternate type=application/rss+xml title="GGfu Personal Study - 學習筆記"><link href=/tags/batch/index.xml rel=feed type=application/rss+xml title="GGfu Personal Study - 學習筆記"><meta property="og:title" content="batch`"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://ggfu0114.github.io/tags/batch/"><meta itemprop=name content="batch`"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="batch`"><meta name=twitter:description content></head><body class="ma0 avenir bg-near-white"><header><div class="pb3-m pb6-l bg-black"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">GGfu Personal Study - 學習筆記</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/categories/ title="Category page">Category</a></li></ul><div class=ananke-socials></div></div></div></nav><div class="tc-l pv3 ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">batch`</h1></div></div></header><main class=pb7 role=main><article class="cf pa3 pa4-m pa4-l"><div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray"><p>Below you will find pages that utilize the taxonomy term “batch`”</p></div></article><div class="mw8 center"><section class="flex-ns flex-wrap justify-around mt5"><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/deploy-s3-bucket-by-cloudformation/ class="link black dim">Deploy s3 bucket by cloudformation</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">預先準備 需要先把 aws cli 給裝起來
利用CLI可以把預先寫好的基礎建設結構佈署到AWS上，下面是簡單部署一個S3的bucket範例。
aws cloudformation deploy --stack-name paulteststack --template-file ./s3-create-template.json --stack-name: 想要deploy到哪一個cloudformation stack裡 --template-file: template 檔案的所在位置 還有很多參數可以設定，詳情可以參考 create-stack
{ "AWSTemplateFormatVersion": "2010-09-09", "Description": "Create S3 bucket on AWS", "Resources": { "S3Test": { "Type": "AWS::S3::Bucket", "Properties": { "BucketName": "paul-test-1495611707" } } } } template 可以依照情境進行更進階的調整，例如調整bucket的CROS讓外部的前端程式可以進行存取。 { "AWSTemplateFormatVersion": "2010-09-09", "Description": "Create S3 bucket on AWS", "Resources": { "S3Test": { "Type": "AWS::S3::Bucket", "Properties": { "BucketName": "paul-test-1495611707", "AccessControl": "PublicReadWrite", "CorsConfiguration": { "CorsRules": [ { "AllowedHeaders": [ "*" ], "AllowedMethods": [ "GET" ], "AllowedOrigins": [ "*" ], "ExposedHeaders": [ "Date" ], "Id": "myCORSRuleId1", "MaxAge": "3600" }, { "AllowedHeaders": [ "x-amz-*" ], "AllowedMethods": [ "DELETE" ], "AllowedOrigins": [ "http://www.</div></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/%E6%94%B6%E9%9B%86-aws-iot-data%E5%88%B0-aws-kinesis-%E8%99%95%E7%90%86%E8%B3%87%E6%96%99%E6%B5%81/ class="link black dim">收集 AWS IoT data到 AWS kinesis 處理資料流</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">使用AWS IoT服務來開發大量的資料傳輸系統，主要是想彙整資料或是做數據分析，AWS同樣提供資料串流的分析服務 kinesis. kinesis服務裡面有三個類別，Data Streams, Data Firehose, Data Analytics. Data Streams: streams 這個服務其實就有點類似 Kafka. 在IoT mqtt的protocol下，publish/subscribe之間系統是不會保存任何傳遞的資料。所以一但需要做資料流分析時，需要將收到的資料存在Message Queue裡，kinesis streams就擔任這個初階的角色，stream可以搭配firhorsec，analytics，lambda&mldr;一起使用。
Data Firehose: firehose可以將stream的資料倒至其他AWS的服務，例如： S3, Redshift&mldr;，firehose服務裡提供transformation功能將資料整理成需要的格式。
Data Analytics: 利用收到的data stream，只要提供需要sql程式，設定好Source(data stream)與Destination，AWS就會提供即時分析服務。
IoT trigger Kinese stream
只要在IoT的Service上建立rule，只要推送資料到設定的topi就會將資料導入Kinese stream。 選定 Send messages to an Kinesis Stream 就可以在符合Rule時將資料導入data stream 選定預先創立好的stream，設定Partition key，還有執行角色就完成 Partition key說明
利用Partition key，IoT的資料可以藉由group的方式去將資料分配的儲存在stream的shards裡面，最容易的方式是用網頁上提供的function: newuuid()，去隨機產出亂數平均的儲存資料，或是利用IoT的資料裡面的Key直做group。 [{'key':'a1', 'value':1}, {'key':'a2', 'value':2}, {'key':'a1', 'value':3}] 如果Partition key填入${key}，那麼第1,3比資料會被送到stream裡的同一個shard，2則被送到另一個shard。
trigger lambda處理收集好的batch資料
shard只由一個lambda服務 pull 多shard由多個lambda執行task, semphore問題</div></div></div></div></section></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://ggfu0114.github.io/>&copy; GGfu Personal Study - 學習筆記 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>